{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from ReplayBuffer import ReplayBuffer as ReplayBuffer\n",
    "from AgentDQN import AgentDQN as AgentDQN\n",
    "\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "import os\n",
    "from IPython.display import clear_output, display\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "MODE = 'VANILA'\n",
    "#MODE = 'RBED'\n",
    "\n",
    "writer = SummaryWriter(comment=MODE)\n",
    "tag_reward = \"reward\"\n",
    "tag_loss = \"loss\"\n",
    "tag_ep = \"epsilon\"\n",
    "\n",
    "discrete_control = ['CartPole-v0', 'MountainCar-v0']\n",
    "continuous_control = ['','']\n",
    "print('hello world')\n",
    "env = gym.make(discrete_control[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "GAMMA      = 0.9 # discount factor\n",
    "EPSILON    = 1\n",
    "EPSILON_DECAY    = 0.99\n",
    "LEARN_RATE = 0.001\n",
    "\n",
    "CHECK_EVERY = 100\n",
    "OPTIMIZE_EVERY = 1\n",
    "\n",
    "STATE_N  = 4\n",
    "ACTION_N = env.action_space.n\n",
    "\n",
    "OPTIMIZE_COUNT = 1\n",
    "\n",
    "NUM_EPISODES = 10000\n",
    "\n",
    "\n",
    "MINREWARD = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "qvfa = AgentDQN(STATE_N, ACTION_N).double().to(device)\n",
    "optimizer = optim.Adam(qvfa.parameters(), lr = LEARN_RATE)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "buffer = ReplayBuffer(1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(state, ep=0):\n",
    "    sample = random.random()\n",
    "    state = torch.from_numpy(state).to(device)\n",
    "    if sample < ep:\n",
    "        return env.action_space.sample()\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            op = qvfa(state)\n",
    "            values, indices = op.max(0)\n",
    "            return indices.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model(i_episode=0):\n",
    "    if buffer.__len__() < BATCH_SIZE:\n",
    "        print(\"optimizing model Not enough samples in buffer : \", buffer.__len__())\n",
    "        return\n",
    "\n",
    "    transitions = buffer.sample(min(BATCH_SIZE, buffer.__len__()))\n",
    "\n",
    "    state_batch = transitions[buffer.header[0]].values\n",
    "    state_batch = torch.from_numpy(np.stack(state_batch, axis=0)).to(device)\n",
    "\n",
    "    action_batch = torch.tensor(transitions[buffer.header[1]].values.tolist()).view(-1, 1).to(device)\n",
    "\n",
    "    next_state_batch = transitions[buffer.header[2]].values\n",
    "    next_state_batch = torch.from_numpy(np.stack(next_state_batch, axis=0)).to(device)\n",
    "\n",
    "    reward_batch = torch.tensor(transitions[buffer.header[3]].values.tolist()).view(-1, 1).to(device)\n",
    "\n",
    "    done_batch = torch.tensor(transitions[buffer.header[4]].values.tolist()).view(-1, 1).to(device)\n",
    "\n",
    "    qsa = qvfa(state_batch).gather(1, action_batch)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        qvfa.eval()\n",
    "        next_state_action_values = qvfa(next_state_batch)\n",
    "        max_next_state_values, _indices = next_state_action_values.max(dim=1)\n",
    "        max_next_state_values = max_next_state_values.view(-1, 1)\n",
    "        next_state_values = ((max_next_state_values * GAMMA).float() + reward_batch).float() * (1 - done_batch).float()\n",
    "        target = next_state_values.double()\n",
    "        qvfa.train()\n",
    "\n",
    "    # ð›¿=ð‘„(ð‘ ,ð‘Ž)âˆ’(ð‘Ÿ+ð›¾maxð‘Žð‘„(ð‘ â€²,ð‘Ž))\n",
    "    optimizer.zero_grad()\n",
    "    loss = criterion(qsa, target)\n",
    "    loss.backward()\n",
    "    # for param in qvfa.parameters():param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()\n",
    "    writer.add_scalar(tag_loss, loss.item(), i_episode)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_decay (episode, highest_reward=0, eps=EPSILON):\n",
    "    eps *= EPSILON_DECAY\n",
    "    return  eps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rbed(episode, highest_reward=0, eps=EPSILON):\n",
    "    return eps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decay_epsilon(episode, highest_reward=0, eps=EPSILON):\n",
    "    if MODE == 'RBED':\n",
    "        eps = rbed(episode, highest_reward, eps)\n",
    "    else:\n",
    "        eps = standard_decay(episode, highest_reward, eps)\n",
    "\n",
    "    eps = eps if eps < EPSILON else EPSILON\n",
    "    return eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i_episode in range(NUM_EPISODES):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        env.render(mode='rgb_array')\n",
    "        action = select_action(state, ep=EPSILON)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        total_reward += reward\n",
    "        if done:\n",
    "            reward = -reward\n",
    "        buffer.insert(state, action, next_state, reward, done)\n",
    "        state = next_state\n",
    "\n",
    "    writer.add_scalar(tag_reward, total_reward, i_episode)\n",
    "    writer.add_scalar(tag_ep, EPSILON, i_episode)\n",
    "    clear()\n",
    "    print('e\\t', EPSILON, '\\t reward', total_reward)\n",
    "    for _ in range(OPTIMIZE_COUNT):\n",
    "        optimize_model(i_episode)\n",
    "\n",
    "    EPSILON = decay_epsilon(i_episode, total_reward, EPSILON)\n",
    "    # if EPSILON > 0.2 and i_episode > 32 and total_reward > MINREWARD:\n",
    "    #     EPSILON -= 0.1\n",
    "    #     MINREWARD += 20\n",
    "\n",
    "print('Complete')\n",
    "env.render()\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
