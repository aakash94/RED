{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from ReplayBuffer import ReplayBuffer as ReplayBuffer\n",
    "from AgentDQN import AgentDQN as AgentDQN\n",
    "\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "import os\n",
    "from IPython.display import clear_output, display\n",
    "\n",
    "\n",
    "\n",
    "import collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "MODE_E = 'EXPONENTIAL'\n",
    "MODE_L = 'LINEAR'\n",
    "MODE_R = 'RBED'\n",
    "\n",
    "MODE = MODE_E\n",
    "\n",
    "writer = SummaryWriter(comment=MODE)\n",
    "tag_reward = \"reward\"\n",
    "tag_loss = \"loss\"\n",
    "tag_ep = \"epsilon\"\n",
    "\n",
    "discrete_control = ['CartPole-v0', 'MountainCar-v0']\n",
    "continuous_control = ['','']\n",
    "print('hello world')\n",
    "env = gym.make(discrete_control[0])\n",
    "\n",
    "\n",
    "scores = deque(maxlen=100)  # because solution depends on last 100 solution.\n",
    "scores.append(int(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "GAMMA      = 0.9 # discount factor\n",
    "EPSILON    = 1\n",
    "EPSILON_DECAY    = 0.999\n",
    "LEARN_RATE = 0.001\n",
    "\n",
    "CHECK_EVERY = 100\n",
    "OPTIMIZE_EVERY = 1\n",
    "\n",
    "STATE_N  = 4\n",
    "ACTION_N = env.action_space.n\n",
    "\n",
    "OPTIMIZE_COUNT = 1\n",
    "\n",
    "NUM_EPISODES = 4096\n",
    "\n",
    "\n",
    "MINREWARD = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "qvfa = AgentDQN(STATE_N, ACTION_N).double().to(device)\n",
    "optimizer = optim.Adam(qvfa.parameters(), lr = LEARN_RATE)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "buffer = ReplayBuffer(1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(state, ep=0):\n",
    "    sample = random.random()\n",
    "    state = torch.from_numpy(state).to(device)\n",
    "    if sample < ep:\n",
    "        return env.action_space.sample()\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            op = qvfa(state)\n",
    "            values, indices = op.max(0)\n",
    "            return indices.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model(i_episode=0):\n",
    "    if buffer.__len__() < BATCH_SIZE:\n",
    "        print(\"optimizing model Not enough samples in buffer : \", buffer.__len__())\n",
    "        return\n",
    "\n",
    "    transitions = buffer.sample(min(BATCH_SIZE, buffer.__len__()))\n",
    "\n",
    "    state_batch = transitions[buffer.header[0]].values\n",
    "    state_batch = torch.from_numpy(np.stack(state_batch, axis=0)).to(device)\n",
    "\n",
    "    action_batch = torch.tensor(transitions[buffer.header[1]].values.tolist()).view(-1, 1).to(device)\n",
    "\n",
    "    next_state_batch = transitions[buffer.header[2]].values\n",
    "    next_state_batch = torch.from_numpy(np.stack(next_state_batch, axis=0)).to(device)\n",
    "\n",
    "    reward_batch = torch.tensor(transitions[buffer.header[3]].values.tolist()).view(-1, 1).to(device)\n",
    "\n",
    "    done_batch = torch.tensor(transitions[buffer.header[4]].values.tolist()).view(-1, 1).to(device)\n",
    "\n",
    "    qsa = qvfa(state_batch).gather(1, action_batch)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        qvfa.eval()\n",
    "        next_state_action_values = qvfa(next_state_batch)\n",
    "        max_next_state_values, _indices = next_state_action_values.max(dim=1)\n",
    "        max_next_state_values = max_next_state_values.view(-1, 1)\n",
    "        next_state_values = ((max_next_state_values * GAMMA).float() + reward_batch).float() * (1 - done_batch).float()\n",
    "        target = next_state_values.double()\n",
    "        qvfa.train()\n",
    "\n",
    "    # ð›¿=ð‘„(ð‘ ,ð‘Ž)âˆ’(ð‘Ÿ+ð›¾maxð‘Žð‘„(ð‘ â€²,ð‘Ž))\n",
    "    optimizer.zero_grad()\n",
    "    loss = criterion(qsa, target)\n",
    "    loss.backward()\n",
    "    # for param in qvfa.parameters():param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()\n",
    "    writer.add_scalar(tag_loss, loss.item(), i_episode)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_solved(value, threshold=195):    \n",
    "    \n",
    "    scores.append(int(value))\n",
    "    score = sum(scores)/100\n",
    "    \n",
    "    if score >= threshold:\n",
    "        print(\"SOLVED\")\n",
    "        return True\n",
    "        \n",
    "    return False\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exponential_decay (eps=EPSILON, decay_rate=EPSILON_DECAY):\n",
    "    # need to decide decay_rate.\n",
    "    eps *= decay_rate\n",
    "    return  eps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_decay( episode, decay_till=2048, start_val = 1.0, end_val = 0.01):\n",
    "    e = episode*((start_val-end_val)/decay_till)\n",
    "    e = start_val - e\n",
    "    return max(e, end_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rbed(episode, highest_reward=0, eps=EPSILON):\n",
    "    return eps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decay_epsilon(episode, highest_reward=0, eps=EPSILON):\n",
    "    \n",
    "    if MODE == MODE_L:\n",
    "        eps = linear_decay(episode, highest_reward, eps)\n",
    "    \n",
    "    elif MODE == MODE_E:\n",
    "        eps = exponential_decay(eps, EPSILON_DECAY)\n",
    "    \n",
    "    elif MODE == MODE_R:\n",
    "        eps = rbed(episode, highest_reward, eps)\n",
    "    \n",
    "    return eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i_episode in range(NUM_EPISODES):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done:        \n",
    "        env.render(mode='rgb_array')\n",
    "        action = select_action(state, ep=EPSILON)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        total_reward += reward\n",
    "        doneV = 0\n",
    "        if done:\n",
    "            reward = -reward\n",
    "            doneV = 1 \n",
    "        \n",
    "        buffer.insert(state, action, next_state, reward, doneV)\n",
    "        state = next_state\n",
    "\n",
    "    writer.add_scalar(tag_reward, total_reward, i_episode)\n",
    "    writer.add_scalar(tag_ep, EPSILON, i_episode)\n",
    "    clear_output()\n",
    "    print('episode number : ',i_episode, '\\te : ', EPSILON, '\\treward : ', total_reward)\n",
    "    \n",
    "    if is_solved(total_reward):\n",
    "        print('solved at episode ', i_episode)\n",
    "        break\n",
    "    \n",
    "    for _ in range(OPTIMIZE_COUNT):\n",
    "        optimize_model(i_episode)\n",
    "\n",
    "    EPSILON = decay_epsilon(i_episode, total_reward, EPSILON)\n",
    "\n",
    "print('Complete')\n",
    "env.render()\n",
    "env.close()\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.render()\n",
    "env.close()\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
