{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from ReplayBuffer import ReplayBuffer as ReplayBuffer\n",
    "from AgentDQN import AgentDQN as AgentDQN\n",
    "\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "import os\n",
    "from IPython.display import clear_output, display\n",
    "\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world\n"
     ]
    }
   ],
   "source": [
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "MODE_E = 'EXPONENTIAL'\n",
    "MODE_L = 'LINEAR'\n",
    "MODE_R = 'RBED_b128_oc8_'\n",
    "\n",
    "\n",
    "MODE = MODE_R\n",
    "\n",
    "writer = SummaryWriter(comment=MODE)\n",
    "tag_reward = \"reward\"\n",
    "tag_loss = \"loss\"\n",
    "tag_ep = \"epsilon\"\n",
    "\n",
    "discrete_control = ['CartPole-v0', 'MountainCar-v0']\n",
    "continuous_control = ['','']\n",
    "print('hello world')\n",
    "env = gym.make(discrete_control[0])\n",
    "\n",
    "\n",
    "scores = deque(maxlen=100)  # because solution depends on last 100 solution.\n",
    "scores.append(int(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "GAMMA      = 0.99 # discount factor\n",
    "EPSILON    = 1\n",
    "EPSILON_DECAY    = 0.999\n",
    "LEARN_RATE = 0.01\n",
    "\n",
    "UPDATE_TARGET_COUNT = 1\n",
    "\n",
    "STATE_N  = 4\n",
    "ACTION_N = env.action_space.n\n",
    "\n",
    "OPTIMIZE_COUNT = 8\n",
    "\n",
    "NUM_EPISODES = 4096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "qvfa = AgentDQN(STATE_N, ACTION_N).double().to(device)\n",
    "q_target = AgentDQN(STATE_N, ACTION_N).double().to(device)\n",
    "optimizer = optim.Adam(qvfa.parameters(), lr = LEARN_RATE)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "buffer = ReplayBuffer(10000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(state, ep=0):\n",
    "    sample = random.random()\n",
    "    state = torch.from_numpy(state).to(device)\n",
    "    if sample < ep:\n",
    "        return env.action_space.sample()\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            qvfa.eval()\n",
    "            op = qvfa(state)\n",
    "            values, indices = op.max(0)\n",
    "            return indices.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model(i_episode=0):\n",
    "    if buffer.__len__() < BATCH_SIZE:\n",
    "        print(\"optimizing model Not enough samples in buffer : \", buffer.__len__())\n",
    "        return\n",
    "\n",
    "    transitions = buffer.sample(min(BATCH_SIZE, buffer.__len__()))\n",
    "\n",
    "    state_batch = transitions[buffer.header[0]].values\n",
    "    state_batch = torch.from_numpy(np.stack(state_batch, axis=0)).to(device)\n",
    "\n",
    "    action_batch = torch.tensor(transitions[buffer.header[1]].values.tolist()).view(-1, 1).to(device)\n",
    "\n",
    "    next_state_batch = transitions[buffer.header[2]].values\n",
    "    next_state_batch = torch.from_numpy(np.stack(next_state_batch, axis=0)).to(device)\n",
    "\n",
    "    reward_batch = torch.tensor(transitions[buffer.header[3]].values.tolist()).view(-1, 1).to(device)\n",
    "\n",
    "    done_batch = torch.tensor(transitions[buffer.header[4]].values.tolist()).view(-1, 1).to(device)\n",
    "    qvfa.train()\n",
    "    qsa = qvfa(state_batch).gather(1, action_batch)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        qvfa.eval()\n",
    "        q_target.eval()\n",
    "        next_state_action_values = q_target(next_state_batch)\n",
    "        max_next_state_values, _indices = next_state_action_values.max(dim=1)\n",
    "        max_next_state_values = max_next_state_values.view(-1, 1)\n",
    "        next_state_values = ((max_next_state_values * GAMMA).float() + reward_batch).float() * (1 - done_batch).float()\n",
    "        target = next_state_values.double()\n",
    "        qvfa.train()\n",
    "\n",
    "    # ð›¿=ð‘„(ð‘ ,ð‘Ž)âˆ’(ð‘Ÿ+ð›¾maxð‘Žð‘„(ð‘ â€²,ð‘Ž))\n",
    "    optimizer.zero_grad()\n",
    "    loss = criterion(qsa, target)\n",
    "    loss.backward()\n",
    "    # for param in qvfa.parameters():param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()\n",
    "    writer.add_scalar(tag_loss, loss.item(), i_episode)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_solved(value, threshold=195):    \n",
    "    \n",
    "    scores.append(int(value))\n",
    "    score = sum(scores)/100\n",
    "    \n",
    "    if score >= threshold:\n",
    "        print(\"SOLVED\")\n",
    "        return True\n",
    "        \n",
    "    return False\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exponential_decay (eps=EPSILON, decay_rate=EPSILON_DECAY):\n",
    "    # need to decide decay_rate.\n",
    "    eps *= decay_rate\n",
    "    return  eps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_decay( episode, decay_till=1024 , start_val = 1.0, end_val = 0.01):\n",
    "    e = (((end_val - start_val)/decay_till)*episode) + start_val\n",
    "    return max(e, end_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "REWARD_THRESHOLD = 0\n",
    "def rbed(current_reward, eps=EPSILON, highest_ep=1.0, stop_red=0.1, lowest_ep=0.01, target_reward=200, target_increment=1):\n",
    "        \n",
    "    global REWARD_THRESHOLD\n",
    "    \n",
    "    \n",
    "#     if eps < lowest_ep:\n",
    "#         return eps\n",
    "    \n",
    "#     if eps < stop_red :\n",
    "#         eps *= decay_rate\n",
    "#         return rps\n",
    "    \n",
    "    steps_to_move_in = target_reward\n",
    "    #steps_to_move_in = steps_to_move\n",
    "    quanta = (highest_ep - lowest_ep)/steps_to_move_in   \n",
    "    \n",
    "    \n",
    "    #REWARD_THRESHOLD = min(REWARD_THRESHOLD, 200)\n",
    "    \n",
    "    if current_reward > REWARD_THRESHOLD :\n",
    "        REWARD_THRESHOLD += target_increment\n",
    "        eps -= quanta\n",
    "        \n",
    "    return max(eps, lowest_ep)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "REWARD_THRESHOLD = 0\n",
    "def red1(current_reward, eps=EPSILON, highest_ep=1.0, lowest_ep=0.01, target_reward=200, target_increment=1, steps_to_move=256):\n",
    "    \n",
    "    steps_to_move_in = target_increment\n",
    "    # steps_to_move_in = steps_to_move\n",
    "    \n",
    "    quanta = (highest_ep - lowest_ep)/steps_to_move_in\n",
    "        \n",
    "    return max(eps, lowest_ep)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decay_epsilon(episode, current_reward=0, eps=EPSILON):\n",
    "    \n",
    "    if MODE == MODE_L:\n",
    "        eps = linear_decay(episode)\n",
    "    \n",
    "    elif MODE == MODE_E:\n",
    "        eps = exponential_decay(eps, EPSILON_DECAY)\n",
    "    \n",
    "    elif MODE == MODE_R:\n",
    "        eps = rbed(current_reward=current_reward, eps=eps)\n",
    "    \n",
    "    return eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode number :  1101 \te :  0.01 \treward :  128.0\n",
      "SOLVED\n",
      "solved at episode  1101\n",
      "Complete\n"
     ]
    }
   ],
   "source": [
    "for i_episode in range(NUM_EPISODES):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    \n",
    "    #1. Avoid moving targets\n",
    "    if i_episode % UPDATE_TARGET_COUNT == 0:\n",
    "        q_target.load_state_dict(qvfa.state_dict())\n",
    "        \n",
    "        \n",
    "    while not done:        \n",
    "        env.render(mode='rgb_array')\n",
    "        action = select_action(state, ep=EPSILON)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        total_reward += reward\n",
    "        doneV = 0\n",
    "        if done:\n",
    "            reward = -reward\n",
    "            doneV = 1 \n",
    "        \n",
    "        buffer.insert(state, action, next_state, reward, doneV)\n",
    "        state = next_state\n",
    "\n",
    "    writer.add_scalar(tag_reward, total_reward, i_episode)\n",
    "    writer.add_scalar(tag_ep, EPSILON, i_episode)\n",
    "    clear_output()\n",
    "    print('episode number : ',i_episode, '\\te : ', EPSILON, '\\treward : ', total_reward)\n",
    "    \n",
    "    if is_solved(total_reward):\n",
    "        print('solved at episode ', i_episode)\n",
    "        break\n",
    "    \n",
    "    for _ in range(OPTIMIZE_COUNT):\n",
    "        optimize_model(i_episode)\n",
    "\n",
    "    EPSILON = decay_epsilon(i_episode, total_reward, EPSILON)\n",
    "\n",
    "print('Complete')\n",
    "env.render()\n",
    "env.close()\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.render()\n",
    "env.close()\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
