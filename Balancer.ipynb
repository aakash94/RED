{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from ReplayBuffer import ReplayBuffer as ReplayBuffer\n",
    "from AgentDQN import AgentDQN as AgentDQN\n",
    "\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "import os\n",
    "from IPython.display import clear_output, display\n",
    "\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world\n"
     ]
    }
   ],
   "source": [
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "MODE_E = 'EXPONENTIAL'\n",
    "MODE_L = 'LINEAR'\n",
    "MODE_R = 'RBED_b128_oc8_'\n",
    "\n",
    "\n",
    "MODE = MODE_R\n",
    "\n",
    "writer = SummaryWriter(comment=MODE)\n",
    "tag_reward = \"reward\"\n",
    "tag_loss = \"loss\"\n",
    "tag_ep = \"epsilon\"\n",
    "\n",
    "discrete_control = ['CartPole-v0', 'MountainCar-v0']\n",
    "continuous_control = ['','']\n",
    "print('hello world')\n",
    "env = gym.make(discrete_control[0])\n",
    "\n",
    "\n",
    "scores = deque(maxlen=100)  # because solution depends on last 100 solution.\n",
    "scores.append(int(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "GAMMA      = 0.99 # discount factor\n",
    "EPSILON    = 1\n",
    "EPSILON_DECAY    = 0.999\n",
    "LEARN_RATE = 0.01\n",
    "\n",
    "UPDATE_TARGET_COUNT = 1\n",
    "\n",
    "STATE_N  = 4\n",
    "ACTION_N = env.action_space.n\n",
    "\n",
    "OPTIMIZE_COUNT = 8\n",
    "\n",
    "NUM_EPISODES = 4096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "qvfa = AgentDQN(STATE_N, ACTION_N).double().to(device)\n",
    "q_target = AgentDQN(STATE_N, ACTION_N).double().to(device)\n",
    "optimizer = optim.Adam(qvfa.parameters(), lr = LEARN_RATE)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "buffer = ReplayBuffer(10000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(state, ep=0):\n",
    "    sample = random.random()\n",
    "    state = torch.from_numpy(state).to(device)\n",
    "    if sample < ep:\n",
    "        return env.action_space.sample()\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            qvfa.eval()\n",
    "            op = qvfa(state)\n",
    "            values, indices = op.max(0)\n",
    "            return indices.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model(i_episode=0):\n",
    "    if buffer.__len__() < BATCH_SIZE:\n",
    "        print(\"optimizing model Not enough samples in buffer : \", buffer.__len__())\n",
    "        return\n",
    "\n",
    "    transitions = buffer.sample(min(BATCH_SIZE, buffer.__len__()))\n",
    "\n",
    "    state_batch = transitions[buffer.header[0]].values\n",
    "    state_batch = torch.from_numpy(np.stack(state_batch, axis=0)).to(device)\n",
    "\n",
    "    action_batch = torch.tensor(transitions[buffer.header[1]].values.tolist()).view(-1, 1).to(device)\n",
    "\n",
    "    next_state_batch = transitions[buffer.header[2]].values\n",
    "    next_state_batch = torch.from_numpy(np.stack(next_state_batch, axis=0)).to(device)\n",
    "\n",
    "    reward_batch = torch.tensor(transitions[buffer.header[3]].values.tolist()).view(-1, 1).to(device)\n",
    "\n",
    "    done_batch = torch.tensor(transitions[buffer.header[4]].values.tolist()).view(-1, 1).to(device)\n",
    "    qvfa.train()\n",
    "    qsa = qvfa(state_batch).gather(1, action_batch)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        qvfa.eval()\n",
    "        q_target.eval()\n",
    "        next_state_action_values = q_target(next_state_batch)\n",
    "        max_next_state_values, _indices = next_state_action_values.max(dim=1)\n",
    "        max_next_state_values = max_next_state_values.view(-1, 1)\n",
    "        next_state_values = ((max_next_state_values * GAMMA).float() + reward_batch).float() * (1 - done_batch).float()\n",
    "        target = next_state_values.double()\n",
    "        qvfa.train()\n",
    "\n",
    "    # 𝛿=𝑄(𝑠,𝑎)−(𝑟+𝛾max𝑎𝑄(𝑠′,𝑎))\n",
    "    optimizer.zero_grad()\n",
    "    loss = criterion(qsa, target)\n",
    "    loss.backward()\n",
    "    # for param in qvfa.parameters():param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()\n",
    "    writer.add_scalar(tag_loss, loss.item(), i_episode)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_solved(value, threshold=195):    \n",
    "    \n",
    "    scores.append(int(value))\n",
    "    score = sum(scores)/100\n",
    "    \n",
    "    if score >= threshold:\n",
    "        print(\"SOLVED\")\n",
    "        return True\n",
    "        \n",
    "    return False\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exponential_decay (eps=EPSILON, decay_rate=EPSILON_DECAY):\n",
    "    # need to decide decay_rate.\n",
    "    eps *= decay_rate\n",
    "    return  eps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_decay( episode, decay_till=1024 , start_val = 1.0, end_val = 0.01):\n",
    "    e = (((end_val - start_val)/decay_till)*episode) + start_val\n",
    "    return max(e, end_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "REWARD_THRESHOLD = 0\n",
    "def rbed(current_reward, eps=EPSILON, highest_ep=1.0, stop_red=0.1, lowest_ep=0.01, target_reward=200, target_increment=1):\n",
    "        \n",
    "    global REWARD_THRESHOLD\n",
    "    \n",
    "    \n",
    "#     if eps < lowest_ep:\n",
    "#         return eps\n",
    "    \n",
    "#     if eps < stop_red :\n",
    "#         eps *= decay_rate\n",
    "#         return rps\n",
    "    \n",
    "    steps_to_move_in = target_reward\n",
    "    #steps_to_move_in = steps_to_move\n",
    "    quanta = (highest_ep - lowest_ep)/steps_to_move_in   \n",
    "    \n",
    "    \n",
    "    #REWARD_THRESHOLD = min(REWARD_THRESHOLD, 200)\n",
    "    \n",
    "    if current_reward > REWARD_THRESHOLD :\n",
    "        REWARD_THRESHOLD += target_increment\n",
    "        eps -= quanta\n",
    "        \n",
    "    return max(eps, lowest_ep)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "REWARD_THRESHOLD = 0\n",
    "def red1(current_reward, eps=EPSILON, highest_ep=1.0, lowest_ep=0.01, target_reward=200, target_increment=1, steps_to_move=256):\n",
    "    \n",
    "    steps_to_move_in = target_increment\n",
    "    # steps_to_move_in = steps_to_move\n",
    "    \n",
    "    quanta = (highest_ep - lowest_ep)/steps_to_move_in\n",
    "        \n",
    "    return max(eps, lowest_ep)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decay_epsilon(episode, current_reward=0, eps=EPSILON):\n",
    "    \n",
    "    if MODE == MODE_L:\n",
    "        eps = linear_decay(episode)\n",
    "    \n",
    "    elif MODE == MODE_E:\n",
    "        eps = exponential_decay(eps, EPSILON_DECAY)\n",
    "    \n",
    "    elif MODE == MODE_R:\n",
    "        eps = rbed(current_reward=current_reward, eps=eps)\n",
    "    \n",
    "    return eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode number :  1101 \te :  0.01 \treward :  128.0\n",
      "SOLVED\n",
      "solved at episode  1101\n",
      "Complete\n"
     ]
    }
   ],
   "source": [
    "for i_episode in range(NUM_EPISODES):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    \n",
    "    #1. Avoid moving targets\n",
    "    if i_episode % UPDATE_TARGET_COUNT == 0:\n",
    "        q_target.load_state_dict(qvfa.state_dict())\n",
    "        \n",
    "        \n",
    "    while not done:        \n",
    "        env.render(mode='rgb_array')\n",
    "        action = select_action(state, ep=EPSILON)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        total_reward += reward\n",
    "        doneV = 0\n",
    "        if done:\n",
    "            reward = -reward\n",
    "            doneV = 1 \n",
    "        \n",
    "        buffer.insert(state, action, next_state, reward, doneV)\n",
    "        state = next_state\n",
    "\n",
    "    writer.add_scalar(tag_reward, total_reward, i_episode)\n",
    "    writer.add_scalar(tag_ep, EPSILON, i_episode)\n",
    "    clear_output()\n",
    "    print('episode number : ',i_episode, '\\te : ', EPSILON, '\\treward : ', total_reward)\n",
    "    \n",
    "    if is_solved(total_reward):\n",
    "        print('solved at episode ', i_episode)\n",
    "        break\n",
    "    \n",
    "    for _ in range(OPTIMIZE_COUNT):\n",
    "        optimize_model(i_episode)\n",
    "\n",
    "    EPSILON = decay_epsilon(i_episode, total_reward, EPSILON)\n",
    "\n",
    "print('Complete')\n",
    "env.render()\n",
    "env.close()\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.render()\n",
    "env.close()\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
